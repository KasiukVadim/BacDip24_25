@article{alistarh2017qsgd,
  title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@misc{gorbunov2019unifiedtheorysgdvariance,
      title={A Unified Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent}, 
      author={Eduard Gorbunov and Filip Hanzely and Peter Richtárik},
      year={2019},
      eprint={1905.11261},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/1905.11261}, 
}

@misc{mishchenko2023distributedlearningcompressedgradient,
      title={Distributed Learning with Compressed Gradient Differences}, 
      author={Konstantin Mishchenko and Eduard Gorbunov and Martin Takáč and Peter Richtárik},
      year={2023},
      eprint={1901.09269},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1901.09269}, 
}

@misc{gorbunov2019unifiedtheorysgdvariance,
      title={A Unified Theory of SGD: Variance Reduction, Sampling, Quantization and Coordinate Descent}, 
      author={Eduard Gorbunov and Filip Hanzely and Peter Richtárik},
      year={2019},
      eprint={1905.11261},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/1905.11261}, 
}
@misc{li2020unifiedanalysisstochasticgradient,
      title={A Unified Analysis of Stochastic Gradient Methods for Nonconvex Federated Optimization}, 
      author={Zhize Li and Peter Richtárik},
      year={2020},
      eprint={2006.07013},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      url={https://arxiv.org/abs/2006.07013}, 
}

@misc{vogels2020powersgdpracticallowrankgradient,
      title={PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization}, 
      author={Thijs Vogels and Sai Praneeth Karimireddy and Martin Jaggi},
      year={2020},
      eprint={1905.13727},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.13727}, 
}


@InProceedings{pmlr-v80-wu18d,
  title = 	 {Error Compensated Quantized {SGD} and its Applications to Large-scale Distributed Optimization},
  author =       {Wu, Jiaxiang and Huang, Weidong and Huang, Junzhou and Zhang, Tong},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5325--5333},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/wu18d/wu18d.pdf},
  url = 	 {https://proceedings.mlr.press/v80/wu18d.html},
  abstract = 	 {Large-scale distributed optimization is of great importance in various applications. For data-parallel based distributed learning, the inter-node gradient communication often becomes the performance bottleneck. In this paper, we propose the error compensated quantized stochastic gradient descent algorithm to improve the training efficiency. Local gradients are quantized to reduce the communication overhead, and accumulated quantization error is utilized to speed up the convergence. Furthermore, we present theoretical analysis on the convergence behaviour, and demonstrate its advantage over competitors. Extensive experiments indicate that our algorithm can compress gradients by a factor of up to two magnitudes without performance degradation.}
}

@misc{shi2024globalmomentumcompressionsparse,
      title={Global Momentum Compression for Sparse Communication in Distributed Learning}, 
      author={Chang-Wei Shi and Shen-Yi Zhao and Yin-Peng Xie and Hao Gao and Wu-Jun Li},
      year={2024},
      eprint={1905.12948},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1905.12948}, 
}
@misc{karimireddy2019errorfeedbackfixessignsgd,
      title={Error Feedback Fixes SignSGD and other Gradient Compression Schemes}, 
      author={Sai Praneeth Karimireddy and Quentin Rebjock and Sebastian U. Stich and Martin Jaggi},
      year={2019},
      eprint={1901.09847},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1901.09847}, 
}
@misc{fatkhullin2021ef21bellswhistles,
      title={EF21 with Bells & Whistles: Practical Algorithmic Extensions of Modern Error Feedback}, 
      author={Ilyas Fatkhullin and Igor Sokolov and Eduard Gorbunov and Zhize Li and Peter Richtárik},
      year={2021},
      eprint={2110.03294},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.03294}, 
}
@misc{beznosikov2024biasedcompressiondistributedlearning,
      title={On Biased Compression for Distributed Learning}, 
      author={Aleksandr Beznosikov and Samuel Horváth and Peter Richtárik and Mher Safaryan},
      year={2024},
      eprint={2002.12410},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.12410}, 
}