# Введение

Применение методов распределенной оптимизации сейчас активно
используется в задачах, которые невозможно эффективно решить на одном
вычислительном устройстве из-за ограничений памяти или вычислительных
мощностей. Кроме того, потребность в более совершенных способах
вычислений только увеличивается с каждым годом.

Важную роль в распределенной оптимизации занимают методы, которые
пытаются сократить расходы на общение между устройствами в процессе
вычислений. Особый интерес представляют операторы сжатия, которые
уменьшают количество передаваемой информации.

Операторы сжатия делятся на 2 группы: смещенные и несмещенные.
Несмещенные давно и хорошо изученные операторы, которые на практике
демонстрируют меньшую производительность, в сравнении с смешенными.
Несмотря на это, лишь сравнительно недавно появилась математическая
теория, описывающая вторых.

## Распределенная оптимизация

Решается задача оптимизации : 
$$\underset{x\in R^d}{min} \ f(x) := \frac{1}{n} \sum_{i=1}^n f_i(x),$$
где $x \in R^d$ - пространство признаков размерности $d$, $n$ -
количество устройст/узлов, $f_i(x) : R^d \rightarrow R$ - потери,
которые получает модель на устройстве под номером $i$. Часто функция
потерь имеем вид :

$$f_i(x) = E_{\xi \sim P_i} [f_{\xi}(x)],$$ где $P_i$ - это
распределение тренировочных данных на устройстве $i$. То, как данные
распределяются между работниками оказывает большое влияние на процесс
обучения.

В данной работе рассматривается случай централизованного типа :

1.  Имеется $n$ работников, каждый производит вычисления градиента
    функции $f_i(x)$( или стохастического градиента) парралельно с
    другими.

2.  Устройства посылают посчитанный градиент на главное устройство -
    сервер

3.  Сервер обрабатывает посчитанный градиент, делает шаг градиентного
    спуска и передает новую информацию каждому устройству, для
    выполнения новый вычислений. И затем процесс повторяется.

Большие временные потери приходятся на общение между сервером и
устройствами. Именно для их уменьшения используется операторы сжатия.

## Базовое решение

Базовым решением задачи является обычный градиентный спуск (GD),
который имеет вид : $$
    x^{k+1} = x^k - \frac{\eta^k}{n} \sum_{i=1}^n \nabla f_i(x^k)$$ где,
$\eta^k > 0$ - темп обучения или размер шага. Базовое решение уже
подвергалось улучшению: использование ускорение(Нестеров), использование
моментума(Метод тяжелого шарика)(Тут вообще общие ускоренные методы),
уменьшение количества обмена информации с сервером, посредством
выполения нескольких шагов локально(Надо найти источник), а также
уменьшения размеров передаваемой информации за счет использования
операторов сжатия(Тут можно вставить Безноса и его
предшественника)(Здесь уже другие подходы к распределенным вычислениям).


# Смещенные операторы

Под операторами сжатия мы подразумеваем (возможно рандомизированное)
отображение $\cC\colon\R^d\to\R^d$ с некоторыми требованиями. Как
правило, в литературе рассматриваются *несмещенные* операторы сжатия
$\cC$ с ограниченным вторым моментом, т.е. :

Пусть $\zeta \geq 1$. Тогда $\cC\in \mathbb{U}(\zeta)$ если $\cC$
явлется несмещенным (то есть, $\Exp{\cC(x)}=x$ для любых $x$) и втором
момент ограничен [^1] $$
 \Exp{ \twonorm{\cC(x)}^2 } \leq \zeta  \twonorm{x}^2, \qquad \forall x\in\R^d \,.$$


# Градиентный спуск с компрессией

Теперь поставим задачу оптимизаци, которую будем решать с помощью
градиентного спуска с сжатием(CGD - compressed gredient descent). Пускай
решается задача: $$
    \underset{x\in R^d}{min} \ f(x)$$, где $f : R^d \rightarrow R$,
$L$ - гладкая и $\mu$-сильно-выпуклая, методом : $$
    x^{k+1} = x^k - \eta C(\nabla f(x))$$, где
$C : R^d \rightarrow R^d$ - оператор сжатия и $\eta > 0$ - темп
обучения.

## Новые операторы сжатия

Предлагается рассмотреть 3 новый оператора сжатия вида :
$$C(\nabla f(x)) = Top_k(\nabla f(x) | w)$$, где $w$ имеет смысл вектора
$"$важности$"$, а $Top_k(\circ | w)$ - выбор $к$ самых важных
координат(обладающих самыми большими весами \"важности$"$ - координатами
$w_i$). В зависимости от того, как определеяется вектор $w$, меняется и
сам оператор.

1.  $w_i := f \left(x^k \right) - f \left(x^k - \eta \left[ \nabla f(x^k) \right]_i \right)$.

    В таком способе определения важности, мы смотрим на сколько мы можем
    уменьшить значение функции спустившить только по одной координате.
    Наибольшей важностью будет обладать координата, которая более других
    минимизирует функцию.

2.  $w := \arg\underset{\substack{w \in \Delta}}{\min} \  f \left(x^k - \eta \sum_{i=1}^d w_i \left[ \nabla f(x^k) \right]_i \right)$.

    Решается задача оптимизации на симплексе, где итоговый вектор $w$
    интерпретируется, как вектор направления наибольшего уменьшения
    функциона в среднем при семплировании из мультиномиального
    распределения индексов с весами $w_i$.

3.  $w := \arg\underset{\substack{w \in [0, 1]^d}}{\min} \  f \left(x^k - \eta \sum_{i=1}^d w_i \left[ \nabla f(x^k) \right]_i \right)$.

    Здесь вектор $w$ \"подбирает\"  размеры шагов относительно спуска,
    будучи играниченным лишь $k$ координатами.



## Анализ полученных результатов

Можно сделать следующие выводы относительно результатов эксперимента

1.  Методы действительно сходятся.

2.  Методы показывают скорость сходимости более высокую, чем $Rand_k$,
    что согласуется с теорией.

3.  Имеет место более высокая скорость сходимости методов (а), (b) и (c)
    в некоторых задачах. Однако они обходят вычислительно дороже, чем
    стандартные.

# Итог

1.  Предложены и опробованы 3 новых оператора сжатия

2.  Получены гарантии сходимости этих методов

3.  Показано превосходство новых методов над стандартными.
